# TwoSix_LLM  
____

Repository for CMSE 495 Experiential Learning in Data Science

**Project Plan Video:**  
https://mediaspace.msu.edu/media/t/1_8ezxk12i  


**Minimum Viable Product Video:**  
https://mediaspace.msu.edu/media/3-17-2024-Sun_TwoSix_MVP_PLAN.mp4/1_qlm596vo 


## Prerequisites
____
### Fine Tuning
Python > 3.10


### UI
Dash > 2.15.0
Python > 3.8

## Objective
____

The objective of the TwoSix_LLM project is to streamline the process of generating fuzzy maps from scientific passages related to offshore fishing/wind turbines. This involves two main tasks: creating a user interface (UI) for data labeling and fine-tuning existing Large Language Models (LLMs) to extract causal relationships from scientific publications. The UI will help researchers label more data which will feed into the finetuning.

### User Interface

The user interface component of the project focuses on designing an intuitive data labeling UI using Dash in Python. This UI minimizes unnecessary interactions while extracting important information from research papers or surveys. It allows researchers to upload documents in various formats and annotate data points with attributes like source, target, and direction. The labeled data is outputted in meticulously structured JSON files for seamless integration with the LLM training process.

### Fine Tuning

The fine tuning aspect involves training existing LLMs to effectively extract relational information from scientific publications. This process utilizes a small portion of LLM weights trained on labeled data generated by the UI. Key techniques under consideration include LoRA and Quantization. The computational intensity of fine tuning necessitates the use of MSUâ€™s High Performance Computing Center (HPCC) for efficient model training. 


## Resources  
All sample data for fine-tuning may be found in Fine_Tuning/LLM_data.  

### Fine Tuning Resources:
- [Llama2](https://llama.meta.com/llama2/)
- [Mistral](https://docs.mistral.ai)
- [Hugging Face](https://huggingface.co)

### UI Resources:
- [Dash](https://github.com/plotly/dash/blob/dev/README.md)
  

## Installation
____

Clone the repository: git clone https://github.com/riggsash/TwoSix_LLM.git

### Fine Tuning:
- Check out [requirements.txt](https://github.com/riggsash/TwoSix_LLM/blob/main/Fine_Tuning/requirements.txt) download in the folder [Fine_Tuning](https://github.com/riggsash/TwoSix_LLM/tree/main/Fine_Tuning)

### UI
- Check out [Install.md](https://github.com/riggsash/TwoSix_LLM/blob/main/UI/INSTALL.md) and [requirements.txt](https://github.com/riggsash/TwoSix_LLM/blob/main/UI/requirements.txt) located in the [UI](https://github.com/riggsash/TwoSix_LLM/tree/main/UI) folder


## Examples
____

### Fine Tune Example:
Check out [mistral_finetune_own_data_tutorial.ipynb](https://github.com/riggsash/TwoSix_LLM/blob/main/Fine_Tuning/mistral_finetune_own_data_tutorial.ipynb) for a working finetuning example. This example uses [OSW_labeled_data.json](https://github.com/riggsash/TwoSix_LLM/blob/main/Fine_Tuning/LLM_data/OSW_labeled_data.json) to fine tune Mistral

### Reproducable Figure:
Check out [Reproducibility.md](https://github.com/riggsash/TwoSix_LLM/blob/main/Fine_Tuning/Reproducibility.md) for instructions on how to make reproducable plot related to the [Fine_Tuning](https://github.com/riggsash/TwoSix_LLM/tree/main/Fine_Tuning) aspect of the project.


## Authors
____
Contributors names and contact info:

* [Ashlin Riggs](https://github.com/riggsash)

* [Spencer Dork](https://github.com/sdork)

* [Aadarsh Swaminathan](https://github.com/swamina9)

* [Riley Millikan](https://github.com/MRMillikan)

* [Krithi Sachithanand](https://github.com/krithi100)
